{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Paste the class file here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "class HR_Data_Prep_Utility(object):\n",
    "\t\"\"\"HR_Data_Prep_Utility is used for preparing data for the ML\"\"\"\n",
    "\n",
    "\tdef __init__(self, dataset, feature_col, target_col, fe_hashing_ratio):\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize and builds the HR Dataset to be used in training a model\n",
    "\t\t\n",
    "\t\tOnly the below features are supported\n",
    "\t\t['MarriedID', 'MaritalStatusID', 'GenderID', 'EmpStatusID', 'DeptID', 'PerfScoreID', 'PayRate', 'Termd', 'PositionID', 'State', 'DOB', 'CitizenDesc', 'HispanicLatino', 'RaceDesc', 'DateofHire', 'DateofTermination', 'ManagerName', 'RecruitmentSource', 'EngagementSurvey', 'EmpSatisfaction', 'SpecialProjectsCount', 'LastPerformanceReview_Date']\n",
    "\n",
    "\t\t:param dataset: pandas dataframe read from csv\n",
    "\t\t:param feature_col: column names of the features\n",
    "\t\t:param target_col: column name of the target\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(HR_Data_Prep_Utility, self).__init__()\n",
    "\t\tself.emp_ds = dataset\n",
    "\t\tself.feature_col = feature_col\n",
    "\t\tself.target_col = target_col\n",
    "\t\tself.fe_hashing_ratio = fe_hashing_ratio\n",
    "\t\tself._cat_col = ['MarriedID', 'MaritalStatusID', 'GenderID','EmpStatusID', 'DeptID', 'Termd', 'PositionID','State', 'CitizenDesc', 'HispanicLatino', 'RaceDesc', 'ManagerName', 'RecruitmentSource']\n",
    "\t\tself._num_col = ['PayRate', 'PerfScoreID', 'Age', 'CurrentCmpyExp', 'EngagementSurvey', 'EmpSatisfaction','SpecialProjectsCount', 'DaysSinceLastRev']\n",
    "\t\tself._cat_col_onehot = ['MarriedID', 'MaritalStatusID', 'GenderID','EmpStatusID', 'Termd', 'CitizenDesc', 'HispanicLatino']\n",
    "\t\tself._cat_columns_feat_hash = ['DeptID', 'PositionID','State', 'RaceDesc', 'ManagerName', 'RecruitmentSource']\n",
    "\n",
    "\n",
    "\tdef get_x_y_data(self):\n",
    "\t\t\"\"\"\n",
    "\t\tDescription\n",
    "\t\t:param name: description\n",
    "\t\t:return: Description\n",
    "\t\t\"\"\"\n",
    "\t\tX, y = self._split_x_y(self.emp_ds)\n",
    "\t\tX = self._values_fix(X)\n",
    "\t\tX = self._add_features(X[self.feature_col])\n",
    "\t\tX = self._missing_values_fix(X)\n",
    "\t\tX = self._encode_category_features(X, reduction_ratio=self.fe_hashing_ratio)\n",
    "\t\ty = self._missing_values_fix(y)\n",
    "\t\ty = self._encode_category_features(y, reduction_ratio=self.fe_hashing_ratio)\n",
    "\t\tX = self._scale_data(X)\n",
    "\t\treturn X, y\n",
    "\n",
    "\n",
    "\tdef _fe_fill_missing_val(self, X, column_name, fe_type):\n",
    "\t\t\"\"\"\n",
    "\t\tDescription\n",
    "\t\t:param name: description\n",
    "\t\t:return: Description\n",
    "\t\t\"\"\"\n",
    "\t\tif(fe_type is 'num'):\n",
    "\t\t\tX[column_name + '_missing'] =  np.zeros((len(X.index), 1))\n",
    "\t\t\t#X.iloc[(X.loc[X[column_name].isna() == True]).index, X.columns.get_loc(column_name + '_missing')] = 1\n",
    "\t\t\tX.loc[X[column_name].isna() == True, column_name + '_missing'] = 1\n",
    "\t\t\t#X.iloc[(X.loc[X[column_name].isna() == True]).index, X.columns.get_loc(column_name)] = 0\n",
    "\t\t\tX.loc[X[column_name].isna() == True, column_name] = 0\n",
    "\t\telif(fe_type is 'str'):\n",
    "\t\t\t#X.iloc[(X.loc[X[column_name].isna() == True]).index, X.columns.get_loc(column_name)] = 'Missing'\n",
    "\t\t\tX.loc[X[column_name].isna() == True, column_name] = 'Missing'\n",
    "\t\treturn X\n",
    "\n",
    "\n",
    "\tdef _fe_category_feature_hashing(self, X, column_name, n_features):\n",
    "\t\t\"\"\"\n",
    "\t\tDescription\n",
    "\t\t:param name: description\n",
    "\t\t:return: Description\n",
    "\t\t\"\"\"\n",
    "\t\tfh = FeatureHasher(n_features=n_features, input_type='string')\n",
    "\t\tx_features_arr = fh.fit_transform(X[column_name].astype('str')).toarray()\n",
    "\t\tcolumn_names = np.array([])\n",
    "\t\tfor i in range(n_features):\n",
    "\t\t\tcolumn_names = np.append(column_names, column_name+'_'+str(i+1))\n",
    "\t\treturn pd.concat([X, pd.DataFrame(x_features_arr, columns=column_names)], axis=1)\n",
    "\n",
    "\n",
    "\tdef _fe_category_one_hot_encoder(self, X, column_name):\n",
    "\t\t\"\"\"\n",
    "\t\tDescription\n",
    "\t\t:param name: description\n",
    "\t\t:return: Description\n",
    "\t\t\"\"\"\n",
    "\t\tx_features_arr = pd.get_dummies(X[column_name])\n",
    "\t\tx_features_arr.rename(columns=lambda x: column_name+'_' + str(x), inplace=True)\n",
    "\t\treturn pd.concat([X, x_features_arr], axis=1)\n",
    "\n",
    "\n",
    "\tdef _split_x_y(self, X):\n",
    "\t\t\"\"\"\n",
    "\t\tDescription\n",
    "\t\t:param name: description\n",
    "\t\t:return: Description\n",
    "\t\t\"\"\"\n",
    "\t\treturn X[self.feature_col], X[self.target_col]\n",
    "\n",
    "\n",
    "\tdef _add_features(self, X):\n",
    "\t\t\"\"\"\n",
    "\t\tDescription\n",
    "\t\t:param name: description\n",
    "\t\t:return: Description\n",
    "\t\t\"\"\"\n",
    "\t\tnow = datetime.now()\n",
    "\t\tif set(['DateofHire','DateofTermination', 'Termd']).issubset(X.columns):\n",
    "\t\t\tX['DateofHire'] = pd.to_datetime(X['DateofHire'], format=\"%m/%d/%Y\")\n",
    "\t\t\tX['DateofTermination'] = pd.to_datetime(X['DateofTermination'], format=\"%m/%d/%y\")\n",
    "\t\t\tX.loc[X['Termd'] == 0, 'CurrentCmpyExp'] = X['DateofHire'].apply(lambda x: now.year - x.year)\n",
    "\t\t\tX.loc[X['Termd'] == 1, 'CurrentCmpyExp'] = (X['DateofTermination'] - X['DateofHire'])/np.timedelta64(1,'Y')\n",
    "\t\t\tX = X.drop(['DateofHire', 'DateofTermination'], axis=1)\n",
    "\t\tif 'LastPerformanceReview_Date' in X.columns:\n",
    "\t\t\tX['LastPerformanceReview_Date'] = pd.to_datetime(X['LastPerformanceReview_Date'], format=\"%m/%d/%Y\")\n",
    "\t\t\tX['DaysSinceLastRev'] = X['LastPerformanceReview_Date'].apply(lambda x: (now - x).days)\n",
    "\t\t\tX = X.drop(['LastPerformanceReview_Date'], axis=1)\n",
    "\t\tif 'DOB' in X.columns:\n",
    "\t\t\tX['DOB'] = pd.to_datetime(X['DOB'], format=\"%d-%m-%Y\")\n",
    "\t\t\tX['Age'] = X['DOB'].apply(lambda x: now.year - x.year)\n",
    "\t\t\tX = X.drop(['DOB'], axis=1)\n",
    "\t\treturn X\n",
    "\n",
    "\n",
    "\tdef _format_date_of_termination(self, X):\n",
    "\t\t\"\"\"\n",
    "\t\tDescription\n",
    "\t\t:param name: description\n",
    "\t\t:return: Description\n",
    "\t\t\"\"\"\n",
    "\t\tpattern1_match = X['DateofTermination'].str.match(pat = '^(0[1-9]|1[012])/(0[1-9]|1[0-9]|2[0-9]|3[01])/([0-9]{2})$')\n",
    "\t\tdates_p1 = pd.to_datetime((X[pattern1_match==True])['DateofTermination'], format=\"%m/%d/%y\")\n",
    "\t\tpattern2_match = X['DateofTermination'].str.match(pat = '^((19|2[0-9])[0-9]{2})/(0[1-9]|1[012])/(0[1-9]|[12][0-9]|3[01])$')\n",
    "\t\tdates_p2 = pd.to_datetime((X[pattern2_match==True])['DateofTermination'], format=\"%Y/%m/%d\")\n",
    "\t\tcombined_dates = dates_p1.append(dates_p2)\n",
    "\t\tX = X.drop(['DateofTermination'], axis=1)\n",
    "\t\tX.at[combined_dates.index, 'DateofTermination'] = combined_dates.values\n",
    "\t\treturn X\n",
    "\n",
    "\n",
    "\tdef _missing_values_fix(self, X):\n",
    "\t\t\"\"\"\n",
    "\t\tDescription\n",
    "\t\t:param name: description\n",
    "\t\t:return: Description\n",
    "\t\t\"\"\"\n",
    "\t\t#Added features are missed from this.. \n",
    "\t\tcat_columns = list(set(self.feature_col) & set(self._cat_col))\n",
    "\t\tnum_columns = list(set(self.feature_col) & set(self._num_col)) + ['CurrentCmpyExp', 'DaysSinceLastRev', 'Age']\n",
    "\t\tfor column in cat_columns:\n",
    "\t\t\tif column in X.columns:\n",
    "\t\t\t\tX = self._fe_fill_missing_val(X, column, 'str')\n",
    "\t\tfor column in num_columns:\n",
    "\t\t\tif column in X.columns:\n",
    "\t\t\t\tX = self._fe_fill_missing_val(X, column, 'num')\n",
    "\t\treturn X\n",
    "\n",
    "\n",
    "\tdef _values_fix(self, X):\n",
    "\t\t\"\"\"\n",
    "\t\tDescription\n",
    "\t\t:param name: description\n",
    "\t\t:return: Description\n",
    "\t\t\"\"\"\n",
    "\t\tif 'HispanicLatino' in X.columns:\n",
    "\t\t\tX.loc[X['HispanicLatino']=='yes', 'HispanicLatino'] = 'Yes'\n",
    "\t\t\tX.loc[X['HispanicLatino']=='no', 'HispanicLatino'] = 'No'\n",
    "\t\treturn X\n",
    "\n",
    "\n",
    "\tdef _encode_category_features(self, X, reduction_ratio):\n",
    "\t\t\"\"\"\n",
    "\t\tDescription\n",
    "\t\t:param name: description\n",
    "\t\t:return: Description\n",
    "\t\t\"\"\"\n",
    "\t\tcat_columns_oh = list(set(self.feature_col) & set(self._cat_col_onehot))\n",
    "\t\tcat_columns_fh = list(set(self.feature_col) & set(self._cat_columns_feat_hash))\n",
    "\t\tfor column in cat_columns_oh:\n",
    "\t\t\tif column in X.columns:\n",
    "\t\t\t\tX = self._fe_category_one_hot_encoder(X, column)\n",
    "\t\tfor column in cat_columns_fh:\n",
    "\t\t\tif column in X.columns:\n",
    "\t\t\t\t#X = self._fe_category_feature_hashing(X, column, int(len(X[column].unique())*reduction_ratio))\n",
    "\t\t\t\tX = self._fe_category_one_hot_encoder(X, column)\n",
    "\t\tdrop_encoded_fe = []\n",
    "\t\tfor column in cat_columns_oh + cat_columns_fh:\n",
    "\t\t\tif column in X.columns:\n",
    "\t\t\t\tdrop_encoded_fe.append(column)\n",
    "\t\tX = X.drop(drop_encoded_fe, axis=1)\n",
    "\t\treturn X\n",
    "\n",
    "\n",
    "\tdef _scale_data(self, X):\n",
    "\t\t\"\"\"\n",
    "\t\tDescription\n",
    "\t\t:param name: description\n",
    "\t\t:return: Description\n",
    "\t\t\"\"\"\n",
    "\t\tscaler = StandardScaler()\n",
    "\t\tscaler.fit(X)\n",
    "\t\treturn pd.DataFrame(scaler.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the csv file.. \n",
    "if it is in local, follow below, if it is google colab, follow 2nd method given by Nusrath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_ds_l = pd.read_csv('human-resources-data-set/HRDataset_v13.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare two lists\n",
    "One - which columns are going to be your X features\n",
    "two - which columns are going to be your y feaute (only one)\n",
    "Create a object for the class intializing these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features = ['MarriedID', 'MaritalStatusID', 'GenderID','EmpStatusID', 'DeptID', 'PayRate', 'Termd', 'PositionID','State', 'DOB', 'CitizenDesc', 'HispanicLatino', 'RaceDesc', 'DateofHire', 'DateofTermination','ManagerName', 'RecruitmentSource', 'EngagementSurvey', 'EmpSatisfaction','SpecialProjectsCount', 'LastPerformanceReview_Date']\n",
    "y_features = ['PerfScoreID']\n",
    "hr_prep = HR_Data_Prep_Utility(emp_ds_l, x_features, y_features, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call get_x_y_data method\n",
    "you will get your X and y to use and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gkailasam\\AppData\\Local\\Continuum\\anaconda3_1\\lib\\site-packages\\pandas\\core\\indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "X, y = hr_prep.get_x_y_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#sss = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "y_train = y_train.to_numpy().reshape((len(y_train),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "lreg = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(lreg, X_train.to_numpy(), y_train, cv = 10, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -3.18 (+/- 3.10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.39582584, -1.1133946 , -2.46524293, -2.04020426, -3.02004671,\n",
       "       -3.95462336, -4.29361705, -1.96958304, -4.72084224, -1.82019116])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('LR',LinearRegression())])))\n",
    "pipelines.append(('ScaledLASSO', Pipeline([('LASSO', Lasso())])))\n",
    "pipelines.append(('ScaledEN', Pipeline([('EN', ElasticNet())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('KNN', KNeighborsRegressor())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('CART', DecisionTreeRegressor())])))\n",
    "pipelines.append(('ScaledGBM', Pipeline([('GBM', GradientBoostingRegressor())])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLR: -5904223725739737717732802560.000000 (17235347310232976688500703232.000000)\n",
      "ScaledLASSO: -0.362985 (0.155056)\n",
      "ScaledEN: -0.362985 (0.155056)\n",
      "ScaledKNN: -0.415514 (0.135344)\n",
      "ScaledCART: -0.444286 (0.192937)\n",
      "ScaledGBM: -0.318079 (0.104817)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=10, random_state=21)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.258015 (0.102285) with: {'n_estimators': 8}\n",
      "-0.256787 (0.102637) with: {'n_estimators': 9}\n",
      "-0.254644 (0.101552) with: {'n_estimators': 10}\n",
      "-0.250457 (0.097476) with: {'n_estimators': 14}\n",
      "-0.251486 (0.098011) with: {'n_estimators': 15}\n",
      "-0.251090 (0.097527) with: {'n_estimators': 16}\n",
      "Best: -0.250457 using {'n_estimators': 14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gkailasam\\AppData\\Local\\Continuum\\anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "param_grid = dict(n_estimators=np.array([8, 9, 10, 14, 15, 16]))\n",
    "model = GradientBoostingRegressor(random_state=21)\n",
    "kfold = KFold(n_splits=10, random_state=21)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, y_train)\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = grid.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2604828430465249"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "scores = mean_squared_error(y_test, y_test_pred)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.156921 (1.491833) with: {'eta0': 0.015, 'max_iter': 1000}\n",
      "-3.115024 (1.461111) with: {'eta0': 0.015, 'max_iter': 1500}\n",
      "-3.128986 (1.457429) with: {'eta0': 0.015, 'max_iter': 2000}\n",
      "-3.075061 (1.563985) with: {'eta0': 0.016, 'max_iter': 1000}\n",
      "-3.108772 (1.442965) with: {'eta0': 0.016, 'max_iter': 1500}\n",
      "-3.243232 (1.729707) with: {'eta0': 0.016, 'max_iter': 2000}\n",
      "-3.495156 (1.758911) with: {'eta0': 0.017, 'max_iter': 1000}\n",
      "-3.590766 (2.108409) with: {'eta0': 0.017, 'max_iter': 1500}\n",
      "-4.060653 (2.243668) with: {'eta0': 0.017, 'max_iter': 2000}\n",
      "-4.043611 (1.951130) with: {'eta0': 0.018, 'max_iter': 1000}\n",
      "-4.942245 (2.484810) with: {'eta0': 0.018, 'max_iter': 1500}\n",
      "-3.877996 (1.576790) with: {'eta0': 0.018, 'max_iter': 2000}\n",
      "-5.642990 (3.720723) with: {'eta0': 0.019, 'max_iter': 1000}\n",
      "-6.201649 (2.781963) with: {'eta0': 0.019, 'max_iter': 1500}\n",
      "-6.118042 (4.560547) with: {'eta0': 0.019, 'max_iter': 2000}\n",
      "Best: -3.075061 using {'eta0': 0.016, 'max_iter': 1000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gkailasam\\AppData\\Local\\Continuum\\anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "param_grid = {'eta0':(0.015, 0.016, 0.017, 0.018, 0.019), 'max_iter':[1000, 1500, 2000]}\n",
    "model = SGDRegressor(tol=1e-3)\n",
    "kfold = KFold(n_splits=10, random_state=21)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=kfold)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
